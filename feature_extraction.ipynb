{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import openslide\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "from utils.file_utils import save_hdf5\n",
    "from dataset_modules.dataset_h5 import Dataset_All_Bags, Whole_Slide_Bag, get_eval_transforms\n",
    "from models import get_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def compute_w_loader(output_path, loader, model, verbose=0):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        output_path: directory to save computed features (.h5 file)\n",
    "        model: pytorch model\n",
    "        verbose: level of feedback\n",
    "    \"\"\"\n",
    "    if verbose > 0:\n",
    "        print('Processing {}: total of {} batches'.format(file_path, len(loader)))\n",
    "\n",
    "    mode = 'w'\n",
    "    for count, data in enumerate(tqdm(loader)):\n",
    "        batch = data['img']\n",
    "        coords = data['coord'].numpy().astype(np.int32)\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():  # Use no_grad context to disable gradient calculation\n",
    "            features = model(batch)\n",
    "\n",
    "        features = features.cpu().numpy()\n",
    "\n",
    "        asset_dict = {'features': features, 'coords': coords}\n",
    "        save_hdf5(output_path, asset_dict, attr_dict=None, mode=mode)\n",
    "        mode = 'a'\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define arguments\n",
    "data_dir = \"path_to_data_directory\"\n",
    "csv_path = \"path_to_csv_file\"\n",
    "feat_dir = \"path_to_feature_directory\"\n",
    "model_name = 'uni_v1'  # or any other model name\n",
    "batch_size = 256\n",
    "slide_ext = '.svs'\n",
    "no_auto_skip = False\n",
    "target_patch_size = 224\n",
    "\n",
    "# Now you can proceed to load data, model, and run the computation using the defined variables above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing dataset')\n",
    "\n",
    "# Define paths\n",
    "csv_path = \"path_to_csv_file\"\n",
    "feat_dir = \"path_to_feature_directory\"\n",
    "data_h5_dir = \"path_to_h5_directory\"\n",
    "data_slide_dir = \"path_to_slide_directory\"\n",
    "slide_ext = \".svs\"\n",
    "\n",
    "bags_dataset = Dataset_All_Bags(csv_path)\n",
    "\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(feat_dir, 'pt_files'), exist_ok=True)\n",
    "os.makedirs(os.path.join(feat_dir, 'h5_files'), exist_ok=True)\n",
    "dest_files = os.listdir(os.path.join(feat_dir, 'pt_files'))\n",
    "\n",
    "model, img_transforms = get_encoder(args.model_name, target_img_size=args.target_patch_size)\n",
    "\n",
    "_ = model.eval()\n",
    "model = model.to(device)\n",
    "total = len(bags_dataset)\n",
    "\n",
    "loader_kwargs = {'num_workers': 8, 'pin_memory': True} if device.type == \"cuda\" else {}\n",
    "\n",
    "for bag_candidate_idx in tqdm(range(total)):\n",
    "    slide_id = bags_dataset[bag_candidate_idx].split(slide_ext)[0]\n",
    "    bag_name = slide_id+'.h5'\n",
    "    h5_file_path = os.path.join(data_h5_dir, 'patches', bag_name)\n",
    "    slide_file_path = os.path.join(data_slide_dir, slide_id+slide_ext)\n",
    "    print('\\nProgress: {}/{}'.format(bag_candidate_idx, total))\n",
    "    print(slide_id)\n",
    "\n",
    "    if not args.no_auto_skip and slide_id+'.pt' in dest_files:\n",
    "        print('Skipped {}'.format(slide_id))\n",
    "        continue \n",
    "\n",
    "    output_path = os.path.join(feat_dir, 'h5_files', bag_name)\n",
    "    time_start = time.time()\n",
    "    wsi = openslide.open_slide(slide_file_path)\n",
    "    dataset = Whole_Slide_Bag_FP(file_path=h5_file_path, \n",
    "                                    wsi=wsi, \n",
    "                                    img_transforms=img_transforms)\n",
    "\n",
    "    loader = DataLoader(dataset=dataset, batch_size=args.batch_size, **loader_kwargs)\n",
    "    output_file_path = compute_w_loader(output_path, loader=loader, model=model, verbose=1)\n",
    "\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print('\\nComputing features for {} took {} s'.format(output_file_path, time_elapsed))\n",
    "\n",
    "    with h5py.File(output_file_path, \"r\") as file:\n",
    "        features = file['features'][:]\n",
    "        print('Features size: ', features.shape)\n",
    "        print('Coordinates size: ', file['coords'].shape)\n",
    "\n",
    "    features = torch.from_numpy(features)\n",
    "    bag_base, _ = os.path.splitext(bag_name)\n",
    "    torch.save(features, os.path.join(feat_dir, 'pt_files', bag_base+'.pt'))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
